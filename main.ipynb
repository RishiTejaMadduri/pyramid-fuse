{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import cv2\n",
    "import tqdm\n",
    "import json\n",
    "import argparse\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from imageio import imwrite\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "import Utils\n",
    "import models\n",
    "import utils_seg\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision import transforms\n",
    "from utils_seg import transforms as local_transforms\n",
    "from base import DataPrefetcher\n",
    "from utils_seg.helpers import colorize_mask\n",
    "from utils_seg.metrics import eval_metrics, AverageMeter\n",
    "from utils_seg.losses import *\n",
    "from tqdm import tqdm\n",
    "import dataloaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'helper'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-06c7dc3f0c64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_argument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-resume'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhelp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Path to the .pth model checkpoint to resume training'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_argument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'--d'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhelp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'indices of GPUs to enable (default: all)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_argument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'--val'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhelper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Perform validation or not'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_argument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'--epoch'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhelper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'No of epochs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_argument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'--momentum'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.99\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhelper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Momentum'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/argparse.py\u001b[0m in \u001b[0;36madd_argument\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1345\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'unknown action \"%s\"'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0maction_class\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1347\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m         \u001b[0;31m# raise an error if the action type is not callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'helper'"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='BiFuse script for 360 Semantic Segmentation!', formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "parser.add_argument('--path', default='', type=str, help='Path of source images')\n",
    "parser.add_argument('--batch_size', default= 16, type=int, help='batch size')\n",
    "parser.add_argument('--checkpoint_dir', default=None, type=str, help='Path to the saving .pth model')\n",
    "parser.add_argument('--log_dir', default=None, type=str, help='Path to the saving .pth model')\n",
    "parser.add_argument('-resume', default=None, type=str, help='Path to the .pth model checkpoint to resume training')\n",
    "parser.add_argument('--d', default=None, type=str, help='indices of GPUs to enable (default: all)')\n",
    "parser.add_argument('--val', default = True, type = bool, helper = 'Perform validation or not')\n",
    "parser.add_argument('--epoch', default = 100, type = int, helper = 'No of epochs')\n",
    "parser.add_argument('--momentum', default = 0.99, type = float, helper = 'Momentum')\n",
    "parser.add_argument('--beta', defualt = 0.01, type = float, helper = 'Beta')\n",
    "parser.add_argument('--decay', deault = 10e-5, type = float,helper = 'weight_decay')\n",
    "parser.add_argument('--lr', defualt = 0.001, type = float, helper = 'learning_rate')\n",
    "parser.add_argument('--val_per_epoch', defualt = 10, type = int, helper = 'validation per epoch')\n",
    "parser.add_argument('--early_stop', defualt = 10, type = int, hlper = 'early stop')\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyData(data.Dataset):\n",
    "    def __init__(self, root):\n",
    "        imgs = os.listdir(root)\n",
    "        self.imgs = [os.path.join(root, k) for k in imgs]\n",
    "        self.transforms = transforms.Compose([\n",
    "            transforms.ToTensor()\n",
    "            ])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.imgs[index]\n",
    "        rgb_img = Image.open(img_path).convert(\"RGB\")\n",
    "        rgb_img = np.array(rgb_img, np.float32) / 255\n",
    "        rgb_img = cv2.resize(rgb_img, (1024, 512), interpolation=cv2.INTER_AREA)\n",
    "        data = self.transforms(rgb_img)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_available_devices(logger):\n",
    "    sys_gpu = torch.cuda.device_count()\n",
    "    if sys_gpu == 0:\n",
    "        self.logger.warning('No GPUs detected, using the CPU')\n",
    "        n_gpu = 0\n",
    "    elif n_gpu > sys_gpu:\n",
    "        self.logger.warning(f'Nbr of GPU requested is {n_gpu} but only {sys_gpu} are available')\n",
    "        n_gpu = sys_gpu\n",
    "\n",
    "    device = torch.device('cuda:0' if n_gpu > 0 else 'cpu')\n",
    "    self.logger.info(f'Detected GPUs: {sys_gpu} Requested: {n_gpu}')\n",
    "    available_gpus = list(range(n_gpu))\n",
    "    return device, available_gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    \n",
    "    train_logger = logger()\n",
    "\n",
    "    train_loader = VOC(args.path, args.batch_size, 'train')\n",
    "    val_loader = VOC(args.path, args.batch_size, 'val')\n",
    "    device, available_gpus = get_available_devices(logger)\n",
    "    model = MyModel(50)\n",
    "    print(f'\\n{model}\\n')\n",
    "    model = torch.nn.DataParallel(model, available_gpus)\n",
    "    model.to(device)\n",
    "    loss = CrossEntropyLoss2d()\n",
    "    print(loss)\n",
    "    # OPTIMIZER\n",
    "    optim_params = [\n",
    "    {'params': model.parameters(), 'lr': args.lr},\n",
    "    ]\n",
    "\n",
    "    optimizer = torch.optim.Adam(optim_params,\n",
    "                             betas=(args.momentum, args.beta),\n",
    "                             weight_decay=args.weight_decay)\n",
    "    \n",
    "#     wrt_mode, wrt_step = 'train_', 0\n",
    "        \n",
    "       \n",
    "        \n",
    "    # TRANSORMS FOR VISUALIZATION\n",
    "#     restore_transform = transforms.Compose([\n",
    "#         local_transforms.DeNormalize(train_loader.MEAN, train_loader.STD),\n",
    "#         transforms.ToPILImage()])\n",
    "#     viz_transform = transforms.Compose([\n",
    "#         transforms.Resize((400, 400)),\n",
    "#         transforms.ToTensor()])\n",
    "\n",
    "#     if device ==  torch.device('cpu'): prefetch = False\n",
    "#     if prefetch:\n",
    "#         train_loader = DataPrefetcher(train_loader, device=self.device)\n",
    "#         val_loader = DataPrefetcher(val_loader, device=self.device)\n",
    "\n",
    "#     torch.backends.cudnn.benchmark = True\n",
    "\n",
    "        \n",
    "#     # MONITORING\n",
    "#     if args.monitor == 'off':\n",
    "#         mnt_mode = 'off'\n",
    "#         mnt_best = 0\n",
    "#     else:\n",
    "#         mnt_mode, mnt_metric = monitor.split()\n",
    "#         assert mnt_mode in ['min', 'max']\n",
    "#         mnt_best = -math.inf if mnt_mode == 'max' else math.inf\n",
    "#         early_stoping = args.early_stop\n",
    "\n",
    "#     # CHECKPOINTS & TENSOBOARD\n",
    "#     start_time = datetime.datetime.now().strftime('%m-%d_%H-%M')\n",
    "#     checkpoint_dir = os.path.join(save_dir, name, start_time)\n",
    "#     helpers.dir_exists(checkpoint_dir)\n",
    "#     config_save_path = os.path.join(checkpoint_dir, 'config.json')\n",
    "#     with open(config_save_path, 'w') as handle:\n",
    "#         json.dump(self.config, handle, indent=4, sort_keys=True)\n",
    "\n",
    "#     writer_dir = os.path.join(log_dir, name, start_time)\n",
    "#     self.writer = tensorboard.SummaryWriter(writer_dir)\n",
    "\n",
    "#     if resume: _resume_checkpoint(resume)\n",
    "\n",
    "#     for epoch in range(start_epoch, epochs+1):\n",
    "#         # RUN TRAIN (AND VAL)\n",
    "#         results = _train_epoch(epoch)\n",
    "#         if do_validation and epoch % args.val_per_epoch == 0:\n",
    "#             results = _valid_epoch(epoch)\n",
    "\n",
    "#             # LOGGING INFO\n",
    "#             logger.info(f'\\n         ## Info for epoch {epoch} ## ')\n",
    "#             for k, v in results.items():\n",
    "#                 logger.info(f'         {str(k):15s}: {v}')\n",
    "\n",
    "#         if train_logger is not None:\n",
    "#             log = {'epoch' : epoch, **results}\n",
    "#             train_logger.add_entry(log)\n",
    "\n",
    "#         # CHECKING IF THIS IS THE BEST MODEL (ONLY FOR VAL)\n",
    "#         if mnt_mode != 'off' and epoch % args.val_per_epoch == 0:\n",
    "#             try:\n",
    "#                 if mnt_mode == 'min': improved = (log[mnt_metric] < mnt_best)\n",
    "#                 else: improved = (log[mnt_metric] > mnt_best)\n",
    "#             except KeyError:\n",
    "#                 logger.warning(f'The metrics being tracked ({mnt_metric}) has not been calculated. Training stops.')\n",
    "#                 break\n",
    "\n",
    "#             if improved:\n",
    "#                 mnt_best = log[mnt_metric]\n",
    "#                 not_improved_count = 0\n",
    "#             else:\n",
    "#                 not_improved_count += 1\n",
    "\n",
    "#             if not_improved_count > early_stoping:\n",
    "#                 logger.info(f'\\nPerformance didn\\'t improve for {early_stoping} epochs')\n",
    "#                 logger.warning('Training Stoped')\n",
    "#                 break\n",
    "\n",
    "#         # SAVE CHECKPOINT\n",
    "#         if epoch % args.save_period == 0:\n",
    "#             self._save_checkpoint(epoch, save_best=self.improved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _save_checkpoint(self, epoch, save_best=False):\n",
    "    state = {\n",
    "        'arch': type(self.model).__name__,\n",
    "        'epoch': epoch,\n",
    "        'state_dict': self.model.state_dict(),\n",
    "        'optimizer': self.optimizer.state_dict(),\n",
    "        'monitor_best': self.mnt_best,\n",
    "        'config': self.config\n",
    "    }\n",
    "    filename = os.path.join(checkpoint_dir, f'checkpoint-epoch{epoch}.pth')\n",
    "    logger.info(f'\\nSaving a checkpoint: {filename} ...') \n",
    "    torch.save(state, filename)\n",
    "\n",
    "    if save_best:\n",
    "        filename = os.path.join(checkpoint_dir, f'best_model.pth')\n",
    "        torch.save(state, filename)\n",
    "        self.logger.info(\"Saving current best: best_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _resume_checkpoint(resume_path):\n",
    "    logger.info(f'Loading checkpoint : {resume_path}')\n",
    "    checkpoint = torch.load(resume_path)\n",
    "\n",
    "    # Load last run info, the model params, the optimizer and the loggers\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    mnt_best = checkpoint['monitor_best']\n",
    "    not_improved_count = 0\n",
    "\n",
    "#     if checkpoint['config']['arch'] != config['arch']:\n",
    "#         self.logger.warning({'Warning! Current model is not the same as the one in the checkpoint'})\n",
    "#     self.model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "#     if checkpoint['config']['optimizer']['type'] != self.config['optimizer']['type']:\n",
    "#         self.logger.warning({'Warning! Current optimizer is not the same as the one in the checkpoint'})\n",
    "#     self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    # if self.lr_scheduler:\n",
    "    #     self.lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
    "\n",
    "    train_logger = checkpoint['logger']\n",
    "    logger.info(f'Checkpoint <{resume_path}> (epoch {start_epoch}) was loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, model, loss, resume, train_loader, num_classes, val_loader=None, train_logger=None, prefetch=True):\n",
    "        super(Trainer, self).__init__(model, loss, resume, train_loader, num_classes, val_loader, train_logger)\n",
    "        \n",
    "        self.wrt_mode, self.wrt_step = 'train_', 0\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        self.model = model\n",
    "        self.loss = loss\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.logger = train_logger\n",
    "        \n",
    "        # TRANSORMS FOR VISUALIZATION\n",
    "        self.restore_transform = transforms.Compose([\n",
    "            local_transforms.DeNormalize(self.train_loader.MEAN, self.train_loader.STD),\n",
    "            transforms.ToPILImage()])\n",
    "        self.viz_transform = transforms.Compose([\n",
    "            transforms.Resize((400, 400)),\n",
    "            transforms.ToTensor()])\n",
    "        \n",
    "        if self.device ==  torch.device('cpu'): prefetch = False\n",
    "        if prefetch:\n",
    "            self.train_loader = DataPrefetcher(train_loader, device=self.device)\n",
    "            self.val_loader = DataPrefetcher(val_loader, device=self.device)\n",
    "\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    def _train_epoch(self, epoch):\n",
    "        self.logger.info('\\n')\n",
    "            \n",
    "        self.model.train()\n",
    "        self.wrt_mode = 'train'\n",
    "\n",
    "        tic = time.time()\n",
    "        self._reset_metrics()\n",
    "        tbar = tqdm(self.train_loader, ncols=130)\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(tbar):\n",
    "            self.data_time.update(time.time() - tic)\n",
    "            #data, target = data.to(self.device), target.to(self.device)\n",
    "            self.lr_scheduler.step(epoch=epoch-1)\n",
    "\n",
    "            # LOSS & OPTIMIZE\n",
    "            self.optimizer.zero_grad()\n",
    "            output = self.model(data)\n",
    "            assert output[0].size()[2:] == target.size()[1:]\n",
    "            assert output[0].size()[1] == self.num_classes \n",
    "            loss = self.loss(output[0], target)\n",
    "            loss += self.loss(output[1], target) * 0.4\n",
    "            output = output[0]\n",
    "\n",
    "            if isinstance(self.loss, torch.nn.DataParallel):\n",
    "                loss = loss.mean()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.total_loss.update(loss.item())\n",
    "\n",
    "            # measure elapsed time\n",
    "            self.batch_time.update(time.time() - tic)\n",
    "            tic = time.time()\n",
    "\n",
    "            # LOGGING & TENSORBOARD\n",
    "            if batch_idx % self.log_step == 0:\n",
    "                self.wrt_step = (epoch - 1) * len(self.train_loader) + batch_idx\n",
    "                self.writer.add_scalar(f'{self.wrt_mode}/loss', loss.item(), self.wrt_step)\n",
    "\n",
    "            # FOR EVAL\n",
    "            seg_metrics = eval_metrics(output, target, self.num_classes)\n",
    "            self._update_seg_metrics(*seg_metrics)\n",
    "            pixAcc, mIoU, _ = self._get_seg_metrics().values()\n",
    "            \n",
    "            # PRINT INFO\n",
    "            tbar.set_description('TRAIN ({}) | Loss: {:.3f} | Acc {:.2f} mIoU {:.2f} | B {:.2f} D {:.2f} |'.format(\n",
    "                                                epoch, self.total_loss.average, \n",
    "                                                pixAcc, mIoU,\n",
    "                                                self.batch_time.average, self.data_time.average))\n",
    "\n",
    "        # METRICS TO TENSORBOARD\n",
    "        seg_metrics = self._get_seg_metrics()\n",
    "        for k, v in list(seg_metrics.items())[:-1]: \n",
    "            self.writer.add_scalar(f'{self.wrt_mode}/{k}', v, self.wrt_step)\n",
    "        for i, opt_group in enumerate(self.optimizer.param_groups):\n",
    "            self.writer.add_scalar(f'{self.wrt_mode}/Learning_rate_{i}', opt_group['lr'], self.wrt_step)\n",
    "            #self.writer.add_scalar(f'{self.wrt_mode}/Momentum_{k}', opt_group['momentum'], self.wrt_step)\n",
    "\n",
    "        # RETURN LOSS & METRICS\n",
    "        log = {'loss': self.total_loss.average,\n",
    "                **seg_metrics}\n",
    "\n",
    "        #if self.lr_scheduler is not None: self.lr_scheduler.step()\n",
    "        return log\n",
    "\n",
    "    def _valid_epoch(self, epoch):\n",
    "        if self.val_loader is None:\n",
    "            self.logger.warning('Not data loader was passed for the validation step, No validation is performed !')\n",
    "            return {}\n",
    "        self.logger.info('\\n###### EVALUATION ######')\n",
    "\n",
    "        self.model.eval()\n",
    "        self.wrt_mode = 'val'\n",
    "\n",
    "        self._reset_metrics()\n",
    "        tbar = tqdm(self.val_loader, ncols=130)\n",
    "        with torch.no_grad():\n",
    "            val_visual = []\n",
    "            for batch_idx, (data, target) in enumerate(tbar):\n",
    "                #data, target = data.to(self.device), target.to(self.device)\n",
    "                # LOSS\n",
    "                output = self.model(data)\n",
    "                loss = self.loss(output, target)\n",
    "                if isinstance(self.loss, torch.nn.DataParallel):\n",
    "                    loss = loss.mean()\n",
    "                self.total_loss.update(loss.item())\n",
    "\n",
    "                seg_metrics = eval_metrics(output, target, self.num_classes)\n",
    "                self._update_seg_metrics(*seg_metrics)\n",
    "\n",
    "                # LIST OF IMAGE TO VIZ (15 images)\n",
    "                if len(val_visual) < 15:\n",
    "                    target_np = target.data.cpu().numpy()\n",
    "                    output_np = output.data.max(1)[1].cpu().numpy()\n",
    "                    val_visual.append([data[0].data.cpu(), target_np[0], output_np[0]])\n",
    "\n",
    "                # PRINT INFO\n",
    "                pixAcc, mIoU, _ = self._get_seg_metrics().values()\n",
    "                tbar.set_description('EVAL ({}) | Loss: {:.3f}, PixelAcc: {:.2f}, Mean IoU: {:.2f} |'.format( epoch,\n",
    "                                                self.total_loss.average,\n",
    "                                                pixAcc, mIoU))\n",
    "\n",
    "            # WRTING & VISUALIZING THE MASKS\n",
    "            val_img = []\n",
    "            palette = self.train_loader.dataset.palette\n",
    "            for d, t, o in val_visual:\n",
    "                d = self.restore_transform(d)\n",
    "                t, o = colorize_mask(t, palette), colorize_mask(o, palette)\n",
    "                d, t, o = d.convert('RGB'), t.convert('RGB'), o.convert('RGB')\n",
    "                [d, t, o] = [self.viz_transform(x) for x in [d, t, o]]\n",
    "                val_img.extend([d, t, o])\n",
    "            val_img = torch.stack(val_img, 0)\n",
    "            val_img = make_grid(val_img.cpu(), nrow=3, padding=5)\n",
    "            self.writer.add_image(f'{self.wrt_mode}/inputs_targets_predictions', val_img, self.wrt_step)\n",
    "\n",
    "            # METRICS TO TENSORBOARD\n",
    "            self.wrt_step = (epoch) * len(self.val_loader)\n",
    "            self.writer.add_scalar(f'{self.wrt_mode}/loss', self.total_loss.average, self.wrt_step)\n",
    "            seg_metrics = self._get_seg_metrics()\n",
    "            for k, v in list(seg_metrics.items())[:-1]: \n",
    "                self.writer.add_scalar(f'{self.wrt_mode}/{k}', v, self.wrt_step)\n",
    "\n",
    "            log = {\n",
    "                'val_loss': self.total_loss.average,\n",
    "                **seg_metrics\n",
    "            }\n",
    "\n",
    "        return log\n",
    "\n",
    "    def _reset_metrics(self):\n",
    "        self.batch_time = AverageMeter()\n",
    "        self.data_time = AverageMeter()\n",
    "        self.total_loss = AverageMeter()\n",
    "        self.total_inter, self.total_union = 0, 0\n",
    "        self.total_correct, self.total_label = 0, 0\n",
    "\n",
    "    def _update_seg_metrics(self, correct, labeled, inter, union):\n",
    "        self.total_correct += correct\n",
    "        self.total_label += labeled\n",
    "        self.total_inter += inter\n",
    "        self.total_union += union\n",
    "\n",
    "    def _get_seg_metrics(self):\n",
    "        pixAcc = 1.0 * self.total_correct / (np.spacing(1) + self.total_label)\n",
    "        IoU = 1.0 * self.total_inter / (np.spacing(1) + self.total_union)\n",
    "        mIoU = IoU.mean()\n",
    "        return {\n",
    "            \"Pixel_Accuracy\": np.round(pixAcc, 3),\n",
    "            \"Mean_IoU\": np.round(mIoU, 3),\n",
    "            \"Class_IoU\": dict(zip(range(self.num_classes), np.round(IoU, 3)))\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
