{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import cv2\n",
    "import tqdm\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import Utils\n",
    "import models\n",
    "import logging\n",
    "import datetime\n",
    "import argparse\n",
    "import utils_seg\n",
    "import dataloaders\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from imageio import imwrite\n",
    "from utils_seg import Logger\n",
    "from utils_seg import helpers\n",
    "from utils_seg.losses import *\n",
    "from dataloaders.voc import VOC\n",
    "from base import DataPrefetcher\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid\n",
    "from models.pyramid_fusion import PyFuse\n",
    "from utils_seg.helpers import colorize_mask\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from utils_seg import transforms as local_transforms\n",
    "from utils_seg.metrics import eval_metrics, AverageMeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'argparse' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-121d7f6d8866>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArgumentParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'BiFuse script for 360 Semantic Segmentation!'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformatter_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArgumentDefaultsHelpFormatter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_argument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'--path'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhelp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Path of source images'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_argument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'--batch_size'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhelp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'batch size'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_argument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'--checkpoint_dir'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhelp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Path to the saving .pth model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_argument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'--log_dir'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhelp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Path to the saving .pth model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'argparse' is not defined"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='BiFuse script for 360 Semantic Segmentation!', formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "parser.add_argument('--path', default='', type=str, help='Path of source images')\n",
    "parser.add_argument('--batch_size', default= 1, type=int, help='batch size')\n",
    "parser.add_argument('--checkpoint_dir', default=None, type=str, help='Path to the saving .pth model')\n",
    "parser.add_argument('--log_dir', default=None, type=str, help='Path to the saving .pth model')\n",
    "parser.add_argument('-resume', default=None, type=str, help='Path to the .pth model checkpoint to resume training')\n",
    "parser.add_argument('--d', default=None, type=str, help='indices of GPUs to enable (default: all)')\n",
    "parser.add_argument('--val', default = True, type = bool, help = 'Perform validation or not')\n",
    "parser.add_argument('--epoch', default = 100, type = int, help = 'No of epochs')\n",
    "parser.add_argument('--momentum', default = 0.99, type = float, help = 'Momentum')\n",
    "parser.add_argument('--beta', default = 0.01, type = float, help = 'Beta')\n",
    "parser.add_argument('--weight_decay', default = 10e-5, type = float,help = 'weight_decay')\n",
    "parser.add_argument('--lr', default = 0.001, type = float, help = 'learning_rate')\n",
    "parser.add_argument('--val_per_epoch', default = 10, type = int, help = 'validation per epoch')\n",
    "parser.add_argument('--early_stop', default = 10, type = int, help = 'early stop')\n",
    "parser.add_argument('--monitor', default = True, type = bool, help = 'monitor params for early stop')\n",
    "parser.add_argument('--n_gpu', default = 1, type = int, help = 'n_gpu')\n",
    "parser.add_argument('--name', default = \"pyfuse\", type = str, help = 'name')\n",
    "parser.add_argument('--log_step', default = 10, type = int, help = 'log_steps')\n",
    "parser.add_argument('--num_classes', default = 21, type = int, help = 'Number of classes')\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MyData(data.Dataset):\n",
    "#     def __init__(self, root):\n",
    "#         imgs = os.listdir(root)\n",
    "#         self.imgs = [os.path.join(root, k) for k in imgs]\n",
    "#         self.transforms = transforms.Compose([\n",
    "#             transforms.ToTensor()\n",
    "#             ])\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         img_path = self.imgs[index]\n",
    "#         rgb_img = Image.open(img_path).convert(\"RGB\")\n",
    "#         rgb_img = np.array(rgb_img, np.float32) / 255\n",
    "#         rgb_img = cv2.resize(rgb_img, (1024, 512), interpolation=cv2.INTER_AREA)\n",
    "#         data = self.transforms(rgb_img)\n",
    "\n",
    "#         return data\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_available_devices(logger, n_gpu):\n",
    "    sys_gpu = torch.cuda.device_count()\n",
    "    if sys_gpu == 0:\n",
    "        logger.warning('No GPUs detected, using the CPU')\n",
    "        n_gpu = 0\n",
    "    elif n_gpu > sys_gpu:\n",
    "        logger.warning(f'Nbr of GPU requested is {n_gpu} but only {sys_gpu} are available')\n",
    "        n_gpu = sys_gpu\n",
    "\n",
    "    device = torch.device('cuda:0' if n_gpu > 0 else 'cpu')\n",
    "    print(f'Detected GPUs: {sys_gpu} Requested: {n_gpu}')\n",
    "    available_gpus = list(range(n_gpu))\n",
    "    return device, available_gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    \n",
    "    logger = Logger()\n",
    "\n",
    "    train_loader = VOC(args.path, args.batch_size, 'train')\n",
    "    print(train_loader)\n",
    "    val_loader = VOC(args.path, args.batch_size, 'val')\n",
    "    device, available_gpus = get_available_devices(logger, args.n_gpu)\n",
    "    model = PyFuse(50)\n",
    "#     print(f'\\n{model}\\n')\n",
    "    model = torch.nn.DataParallel(model, available_gpus)\n",
    "    model.to(device)\n",
    "    loss = CrossEntropyLoss2d()\n",
    "    print(loss)\n",
    "    # OPTIMIZER\n",
    "    optim_params = [\n",
    "    {'params': model.parameters(), 'lr': args.lr},\n",
    "    ]\n",
    "\n",
    "    optimizer = torch.optim.Adam(optim_params,\n",
    "                             betas=(args.momentum, args.beta),\n",
    "                             weight_decay=(args.weight_decay))\n",
    "    \n",
    "    wrt_mode, wrt_step = 'train_', 0\n",
    "        \n",
    "       \n",
    "        \n",
    "#     TRANSORMS FOR VISUALIZATION\n",
    "    restore_transform = transforms.Compose([\n",
    "        local_transforms.DeNormalize(train_loader.MEAN, train_loader.STD),\n",
    "        transforms.ToPILImage()])\n",
    "    viz_transform = transforms.Compose([\n",
    "        transforms.Resize((400, 400)),\n",
    "        transforms.ToTensor()])\n",
    "    \n",
    "    train_loader = restore_transform(train_loader)\n",
    "    val_loader = restore_transform(val_loader)\n",
    "    prefetch = True\n",
    "    if device ==  torch.device('cpu'): \n",
    "        prefetch = False\n",
    "    if prefetch:\n",
    "        train_loader = DataPrefetcher(train_loader, device=device)\n",
    "        val_loader = DataPrefetcher(val_loader, device=device)\n",
    "\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    monitor = \"max Mean_IoU\"\n",
    "        \n",
    "    # MONITORING\n",
    "    if args.monitor == 'False':\n",
    "        mnt_mode = 'off'\n",
    "        mnt_best = 0\n",
    "    else:\n",
    "        mnt_mode, mnt_metric = monitor.split()\n",
    "        assert mnt_mode in ['min', 'max']\n",
    "        mnt_best = -math.inf if mnt_mode == 'max' else math.inf\n",
    "        early_stoping = args.early_stop\n",
    "\n",
    "    # CHECKPOINTS & TENSOBOARD\n",
    "    start_time = datetime.datetime.now().strftime('%m-%d_%H-%M')\n",
    "    checkpoint_dir = os.path.join(args.checkpoint_dir, args.name, start_time)\n",
    "    helpers.dir_exists(checkpoint_dir)\n",
    "    config_save_path = os.path.join(checkpoint_dir, 'config.json')\n",
    "#     with open(config_save_path, 'w') as handle:\n",
    "#         json.dump(config, handle, indent=4, sort_keys=True)\n",
    "\n",
    "    writer_dir = os.path.join(args.log_dir, args.name, start_time)\n",
    "    writer = SummaryWriter(writer_dir)\n",
    "\n",
    "    if args.resume: _resume_checkpoint(args.resume)\n",
    "    start_epoch = 1\n",
    "    epoch=args.epoch\n",
    "    num_classes = args.num_classes\n",
    "    for epoch in range(start_epoch, epoch+1):\n",
    "        # RUN TRAIN (AND VAL)\n",
    "        pdb.st_trace()\n",
    "        results = _train_epoch(model,epoch, num_classes, train_loader, logger, writer, optimizer)\n",
    "        if do_validation and epoch % args.val_per_epoch == 0:\n",
    "            results = _valid_epoch(model, epoch)\n",
    "\n",
    "            # LOGGING INFO\n",
    "            logger.info(f'\\n         ## Info for epoch {epoch} ## ')\n",
    "            for k, v in results.items():\n",
    "                logger.info(f'         {str(k):15s}: {v}')\n",
    "\n",
    "        if train_logger is not None:\n",
    "            log = {'epoch' : epoch, **results}\n",
    "            train_logger.add_entry(log)\n",
    "\n",
    "        # CHECKING IF THIS IS THE BEST MODEL (ONLY FOR VAL)\n",
    "        if mnt_mode != 'off' and epoch % args.val_per_epoch == 0:\n",
    "            try:\n",
    "                if mnt_mode == 'min': improved = (log[mnt_metric] < mnt_best)\n",
    "                else: improved = (log[mnt_metric] > mnt_best)\n",
    "            except KeyError:\n",
    "                logger.warning(f'The metrics being tracked ({mnt_metric}) has not been calculated. Training stops.')\n",
    "                break\n",
    "\n",
    "            if improved:\n",
    "                mnt_best = log[mnt_metric]\n",
    "                not_improved_count = 0\n",
    "            else:\n",
    "                not_improved_count += 1\n",
    "\n",
    "            if not_improved_count > early_stoping:\n",
    "                logger.info(f'\\nPerformance didn\\'t improve for {early_stoping} epochs')\n",
    "                logger.warning('Training Stoped')\n",
    "                break\n",
    "\n",
    "        # SAVE CHECKPOINT\n",
    "        if epoch % args.save_period == 0:\n",
    "            _save_checkpoint(epoch, save_best=improved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _save_checkpoint(epoch, save_best=False):\n",
    "    state = {\n",
    "        'arch': type(self.model).__name__,\n",
    "        'epoch': epoch,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'monitor_best': mnt_best,\n",
    "    }\n",
    "    filename = os.path.join(checkpoint_dir, f'checkpoint-epoch{epoch}.pth')\n",
    "    logger.info(f'\\nSaving a checkpoint: {filename} ...') \n",
    "    torch.save(state, filename)\n",
    "\n",
    "    if save_best:\n",
    "        filename = os.path.join(checkpoint_dir, f'best_model.pth')\n",
    "        torch.save(state, filename)\n",
    "        self.logger.info(\"Saving current best: best_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _resume_checkpoint(resume_path):\n",
    "    logger.info(f'Loading checkpoint : {resume_path}')\n",
    "    checkpoint = torch.load(resume_path)\n",
    "\n",
    "    # Load last run info, the model params, the optimizer and the loggers\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    mnt_best = checkpoint['monitor_best']\n",
    "    not_improved_count = 0\n",
    "\n",
    "#     if checkpoint['config']['arch'] != config['arch']:\n",
    "#         self.logger.warning({'Warning! Current model is not the same as the one in the checkpoint'})\n",
    "#     self.model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "#     if checkpoint['config']['optimizer']['type'] != self.config['optimizer']['type']:\n",
    "#         self.logger.warning({'Warning! Current optimizer is not the same as the one in the checkpoint'})\n",
    "#     self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    # if self.lr_scheduler:\n",
    "    #     self.lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
    "\n",
    "    train_logger = checkpoint['logger']\n",
    "    logger.info(f'Checkpoint <{resume_path}> (epoch {start_epoch}) was loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _train_epoch(model,epoch, num_classes, train_loader, logger, writer, optimizer):\n",
    "#     logger.info('\\n')\n",
    "\n",
    "    model.train()\n",
    "    wrt_mode = 'train'\n",
    "\n",
    "    tic = time.time()\n",
    "    _reset_metrics()\n",
    "    tbar = tqdm(train_loader, ncols=130)\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(tbar):\n",
    "        data_time.update(time.time() - tic)\n",
    "        #data, target = data.to(self.device), target.to(self.device)\n",
    "        lr_scheduler.step(epoch=epoch-1)\n",
    "\n",
    "        # LOSS & OPTIMIZE\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        assert output[0].size()[2:] == target.size()[1:]\n",
    "        assert output[0].size()[1] == num_classes \n",
    "        loss = loss(output[0], target)\n",
    "        loss += loss(output[1], target) * 0.4\n",
    "        output = output[0]\n",
    "\n",
    "        if isinstance(loss, torch.nn.DataParallel):\n",
    "            loss = loss.mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss.update(loss.item())\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - tic)\n",
    "        tic = time.time()\n",
    "\n",
    "        # LOGGING & TENSORBOARD\n",
    "        if batch_idx % log_step == 0:\n",
    "            wrt_step = (epoch - 1) * len(train_loader) + batch_idx\n",
    "            writer.add_scalar(f'{wrt_mode}/loss', loss.item(), wrt_step)\n",
    "\n",
    "        # FOR EVAL\n",
    "        seg_metrics = eval_metrics(output, target, num_classes)\n",
    "        _update_seg_metrics(*seg_metrics)\n",
    "        pixAcc, mIoU, _ = _get_seg_metrics().values()\n",
    "\n",
    "        # PRINT INFO\n",
    "        tbar.set_description('TRAIN ({}) | Loss: {:.3f} | Acc {:.2f} mIoU {:.2f} | B {:.2f} D {:.2f} |'.format(\n",
    "                                            epoch, total_loss.average, \n",
    "                                            pixAcc, mIoU,\n",
    "                                            batch_time.average, data_time.average))\n",
    "\n",
    "    # METRICS TO TENSORBOARD\n",
    "    seg_metrics = _get_seg_metrics()\n",
    "    for k, v in list(seg_metrics.items())[:-1]: \n",
    "        writer.add_scalar(f'{wrt_mode}/{k}', v, wrt_step)\n",
    "    for i, opt_group in enumerate(optimizer.param_groups):\n",
    "        writer.add_scalar(f'{wrt_mode}/Learning_rate_{i}', opt_group['lr'], wrt_step)\n",
    "        #self.writer.add_scalar(f'{self.wrt_mode}/Momentum_{k}', opt_group['momentum'], self.wrt_step)\n",
    "\n",
    "    # RETURN LOSS & METRICS\n",
    "    log = {'loss': total_loss.average,\n",
    "            **seg_metrics}\n",
    "\n",
    "    #if self.lr_scheduler is not None: self.lr_scheduler.step()\n",
    "    return log\n",
    "\n",
    "def _valid_epoch(model, epoch):\n",
    "    if val_loader is None:\n",
    "        logger.warning('Not data loader was passed for the validation step, No validation is performed !')\n",
    "        return {}\n",
    "    logger.info('\\n###### EVALUATION ######')\n",
    "\n",
    "    model.eval()\n",
    "    wrt_mode = 'val'\n",
    "\n",
    "    _reset_metrics()\n",
    "    tbar = tqdm(val_loader, ncols=130)\n",
    "    with torch.no_grad():\n",
    "        val_visual = []\n",
    "        for batch_idx, (data, target) in enumerate(tbar):\n",
    "            #data, target = data.to(self.device), target.to(self.device)\n",
    "            # LOSS\n",
    "            output = model(data)\n",
    "            loss = loss(output, target)\n",
    "            if isinstance(loss, torch.nn.DataParallel):\n",
    "                loss = loss.mean()\n",
    "            total_loss.update(loss.item())\n",
    "\n",
    "            seg_metrics = eval_metrics(output, target, num_classes)\n",
    "            _update_seg_metrics(*seg_metrics)\n",
    "\n",
    "            # LIST OF IMAGE TO VIZ (15 images)\n",
    "            if len(val_visual) < 15:\n",
    "                target_np = target.data.cpu().numpy()\n",
    "                output_np = output.data.max(1)[1].cpu().numpy()\n",
    "                val_visual.append([data[0].data.cpu(), target_np[0], output_np[0]])\n",
    "\n",
    "            # PRINT INFO\n",
    "            pixAcc, mIoU, _ = _get_seg_metrics().values()\n",
    "            tbar.set_description('EVAL ({}) | Loss: {:.3f}, PixelAcc: {:.2f}, Mean IoU: {:.2f} |'.format( epoch,\n",
    "                                            total_loss.average,\n",
    "                                            pixAcc, mIoU))\n",
    "\n",
    "        # WRTING & VISUALIZING THE MASKS\n",
    "        val_img = []\n",
    "        palette = train_loader.dataset.palette\n",
    "        for d, t, o in val_visual:\n",
    "            d = restore_transform(d)\n",
    "            t, o = colorize_mask(t, palette), colorize_mask(o, palette)\n",
    "            d, t, o = d.convert('RGB'), t.convert('RGB'), o.convert('RGB')\n",
    "            [d, t, o] = [viz_transform(x) for x in [d, t, o]]\n",
    "            val_img.extend([d, t, o])\n",
    "        val_img = torch.stack(val_img, 0)\n",
    "        val_img = make_grid(val_img.cpu(), nrow=3, padding=5)\n",
    "        writer.add_image(f'{wrt_mode}/inputs_targets_predictions', val_img, wrt_step)\n",
    "\n",
    "        # METRICS TO TENSORBOARD\n",
    "        wrt_step = (epoch) * len(val_loader)\n",
    "        writer.add_scalar(f'{wrt_mode}/loss', total_loss.average, wrt_step)\n",
    "        seg_metrics = _get_seg_metrics()\n",
    "        for k, v in list(seg_metrics.items())[:-1]: \n",
    "            writer.add_scalar(f'{wrt_mode}/{k}', v, wrt_step)\n",
    "\n",
    "        log = {\n",
    "            'val_loss': total_loss.average,\n",
    "            **seg_metrics\n",
    "        }\n",
    "\n",
    "    return log\n",
    "\n",
    "def _reset_metrics():\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    total_loss = AverageMeter()\n",
    "    total_inter, total_union = 0, 0\n",
    "    total_correct, total_label = 0, 0\n",
    "\n",
    "def _update_seg_metrics(correct, labeled, inter, union):\n",
    "    total_correct += correct\n",
    "    total_label += labeled\n",
    "    total_inter += inter\n",
    "    total_union += union\n",
    "\n",
    "def _get_seg_metrics(total_correct, total_label, total_inter, total_union):\n",
    "    pixAcc = 1.0 *total_correct / (np.spacing(1) + total_label)\n",
    "    IoU = 1.0 * total_inter / (np.spacing(1) + total_union)\n",
    "    mIoU = IoU.mean()\n",
    "    return {\n",
    "        \"Pixel_Accuracy\": np.round(pixAcc, 3),\n",
    "        \"Mean_IoU\": np.round(mIoU, 3),\n",
    "        \"Class_IoU\": dict(zip(range(self.num_classes), np.round(IoU, 3)))\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
